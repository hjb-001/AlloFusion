{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "# Install tensorflow if not already installed\n",
    "# %pip install tensorflow\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "with open(\"features_data/diversity/train_dataset_1.pkl\", 'rb') as file:\n",
    "    positive_set = pickle.load(file)\n",
    "\n",
    "with open(\"features_data/diversity/train_dataset_0.pkl\", 'rb') as file:\n",
    "    negative_set_entire = pickle.load(file)\n",
    "column_names = ['pdb_name','residue','features','label']\n",
    "# 确保 positive_set 和 negative_set_entire 是 DataFrame\n",
    "if isinstance(positive_set, dict):\n",
    "    positive_set = pd.DataFrame.from_dict(positive_set)\n",
    "if isinstance(negative_set_entire, dict):\n",
    "    negative_set_entire = pd.DataFrame.from_dict(negative_set_entire)\n",
    "# randomly pick negative samples to balance it with positve samples (1.5x positive samples)\n",
    "Negative_Samples = negative_set_entire.sample(n=round(len(positive_set)*15), random_state=42)\n",
    "\n",
    "# combine positive and negative sets to make the final dataset\n",
    "Train_set = pd.concat([positive_set, Negative_Samples], ignore_index=True, axis=0)\n",
    "\n",
    "# collect the features and labels of train set\n",
    "np.set_printoptions(suppress=True)\n",
    "X_val = [0]*len(Train_set)\n",
    "for i in range(len(Train_set)):\n",
    "    feat = Train_set['features'][i]\n",
    "    # 提取T5特征和bio特征\n",
    "    # feat = np.concatenate((feat[:1024],feat[1044:]))\n",
    "    X_val[i] = feat\n",
    "X_train_orig = np.asarray(X_val)\n",
    "y_val = Train_set['label'].to_numpy(dtype=float)\n",
    "Y_train_orig = y_val.reshape(y_val.size,1)\n",
    "\n",
    "# Generate a random order of elements with np.random.permutation and simply index into the arrays Feature and label \n",
    "idx = np.random.permutation(len(X_train_orig))\n",
    "X_train,Y_train = X_train_orig[idx], Y_train_orig[idx]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # fit on training set only\n",
    "X_train = scaler.transform(X_train) # apply transform to the training set\n",
    "\n",
    "# load test data\n",
    "with open(\"features_data/diversity/test_dataset.pkl\", 'rb') as file:\n",
    "    Independent_test_set = pickle.load(file)\n",
    "\n",
    "if isinstance(Independent_test_set, dict):\n",
    "    Independent_test_set = pd.DataFrame.from_dict(Independent_test_set)\n",
    "# collect the features and labels for independent set\n",
    "X_independent = [0]*len(Independent_test_set)\n",
    "for i in range(len(Independent_test_set)):\n",
    "    feat1 = Independent_test_set['features'][i]\n",
    "    # feat1 = Independent_test_set['features'][i]\n",
    "    # feat1 = np.concatenate((feat1[:1024],feat1[1044:]))\n",
    "    X_independent[i] = feat1\n",
    "X_test = np.asarray(X_independent)\n",
    "y_independent = Independent_test_set['label'].to_numpy(dtype=float)\n",
    "Y_test = y_independent.reshape(y_independent.size,1)\n",
    "X_test = scaler.transform(X_test) # apply standardization (transform) to the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from kerastuner import Objective\n",
    "import keras_tuner\n",
    "\n",
    "feat_shape = X_train[0].size\n",
    "# 定义CNN模型，接收超参数\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # 第一层卷积层\n",
    "        model.add(tfl.Conv1D(\n",
    "            filters=hp.Int('conv1_filters', min_value=32, max_value=128, step=32), \n",
    "            kernel_size=hp.Int('conv1_kernel_size', min_value=3, max_value=7, step=2),\n",
    "            activation='relu',\n",
    "            input_shape=(feat_shape, 1)\n",
    "        ))\n",
    "        model.add(tfl.BatchNormalization())\n",
    "        model.add(tfl.Dropout(rate=hp.Float('dropout1_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # 第二层卷积层\n",
    "        model.add(tfl.Conv1D(\n",
    "            filters=hp.Int('conv2_filters', min_value=64, max_value=256, step=64),\n",
    "            kernel_size=hp.Int('conv2_kernel_size', min_value=3, max_value=7, step=2),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(tfl.BatchNormalization())\n",
    "        model.add(tfl.Dropout(rate=hp.Float('dropout2_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # 第三层卷积层\n",
    "        model.add(tfl.Conv1D(\n",
    "            filters=hp.Int('conv3_filters', min_value=32, max_value=128, step=32),\n",
    "            kernel_size=hp.Int('conv3_kernel_size', min_value=3, max_value=7, step=2),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(tfl.BatchNormalization())\n",
    "        model.add(tfl.Dropout(rate=hp.Float('dropout3_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "         # 新增的第四层卷积层\n",
    "        model.add(tfl.Conv1D(\n",
    "            filters=hp.Int('conv4_filters', min_value=32, max_value=128, step=32),\n",
    "            kernel_size=hp.Int('conv4_kernel_size', min_value=3, max_value=7, step=2),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(tfl.BatchNormalization())\n",
    "        model.add(tfl.Dropout(rate=hp.Float('dropout4_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Flatten层\n",
    "        model.add(tfl.Flatten())\n",
    "\n",
    "        # 全连接层\n",
    "        model.add(tfl.Dense(\n",
    "            units=hp.Int('dense_units', min_value=64, max_value=256, step=64), \n",
    "            activation='relu'\n",
    "        ))\n",
    "\n",
    "        # 输出层\n",
    "        model.add(tfl.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # 编译模型\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "            ),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['AUC']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "# 选择贝叶斯优化调优器\n",
    "tuner = BayesianOptimization(\n",
    "    CNNHyperModel(),\n",
    "    objective=Objective(\"val_auc\", direction=\"max\"),  # 优化目标\n",
    "    max_trials=10,  # 最大试验次数\n",
    "    executions_per_trial=1,  # 每个试验执行一次\n",
    "    directory='keras_tuner_dir',  # 存储日志的目录\n",
    "    project_name='cnn_hyperparam_tuning4'  # 项目名称\n",
    ")\n",
    "\n",
    "# 调整模型的超参数\n",
    "tuner.search(\n",
    "    X_train, Y_train,  # 训练数据\n",
    "    epochs=10,  # 训练轮数\n",
    "    validation_data=(X_test, Y_test),  # 验证数据\n",
    "    batch_size=32  # 批大小\n",
    ")\n",
    "\n",
    "# 获取最佳超参数组合\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters.values)\n",
    "\n",
    "# 使用最佳超参数训练最终模型\n",
    "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
    "best_model.fit(X_train,Y_train, epochs=10, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Model():\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tfl.Conv1D(32, 3, padding='same', activation='relu', input_shape=(feat_shape,1)))\n",
    "    model.add(tfl.BatchNormalization())\n",
    "    model.add(tfl.Dropout(0.2)) # 0.23\n",
    "\n",
    "    model.add(tfl.Conv1D(128, 3, padding='same',activation='relu'))\n",
    "    model.add(tfl.BatchNormalization())\n",
    "    model.add(tfl.Dropout(0.3)) # 0.21\n",
    "\n",
    "    model.add(tfl.Conv1D(32, 5, padding='same',activation='relu'))\n",
    "    model.add(tfl.BatchNormalization()) \n",
    "    model.add(tfl.Dropout(0.2)) # 0.47\n",
    "\n",
    "    model.add(tfl.Conv1D(32, 3, padding='same',activation='relu'))\n",
    "    model.add(tfl.BatchNormalization()) \n",
    "    model.add(tfl.Dropout(0.3)) # 0.47\n",
    "\n",
    "    model.add(tfl.Flatten())\n",
    "    model.add(tfl.Dense(128, activation='relu'))\n",
    "    # model.add(tfl.Dropout(0.5))\n",
    "\n",
    "    model.add(tfl.Dense(32, activation='relu'))\n",
    "    model.add(tfl.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "#{'conv1_filters': 32, 'conv1_kernel_size': 3, 'dropout1_rate': 0.4, 'conv2_filters': 192, 'conv2_kernel_size': 5, 'dropout2_rate': 0.2, 'conv3_filters': 128, 'conv3_kernel_size': 5, 'dropout3_rate': 0.30000000000000004, 'dense_units': 128, 'learning_rate': 0.00015872369686433261\n",
    "# {'conv1_filters': 32, 'conv1_kernel_size': 3, 'dropout1_rate': 0.2, 'conv2_filters': 128, 'conv2_kernel_size': 3, 'dropout2_rate': 0.30000000000000004, 'conv3_filters': 32, 'conv3_kernel_size': 5, 'dropout3_rate': 0.2, 'conv4_filters': 32, 'conv4_kernel_size': 3, 'dropout4_rate': 0.30000000000000004, 'dense_units': 128, 'learning_rate': 0.000735323218543868}\n",
    "\n",
    "\n",
    "feat_shape = X_train[0].size\n",
    "# print(\"feat_shape\",feat_shape)\n",
    "cnn_model = CNN_Model()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "cnn_model.compile(optimizer=optimizer,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['AUC', 'accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train the Model\n",
    "batch_size = 32 # 32\n",
    "epochs = 100\n",
    "# 学习率调度器： ReduceLROnPlateau\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_auc', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    min_lr=1e-5, \n",
    "    verbose=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"myModel/multy1/embedding-pssm-bio15.h5\", save_best_only=True) # save the best model weights 仅保存验证集上性能最好的最佳模型权重\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True,verbose=1) # stop training if the validation AUC does not improve for 3 epochs\n",
    "history = cnn_model.fit(X_train , Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[checkpoint, early_stopping, lr_scheduler])\n",
    "\n",
    "df_loss_auc = pd.DataFrame(history.history)\n",
    "\n",
    "# 创建副本并重命名列\n",
    "df_loss= df_loss_auc[['loss','val_loss']].copy()\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    " \n",
    "df_auc= df_loss_auc[['auc','val_auc']].copy()\n",
    "df_auc.rename(columns={'auc':'train','val_auc':'validation'},inplace=True)\n",
    "\n",
    "# 绘制损失和 AUC 曲线\n",
    "Model_Loss_plot_title = 'Model Loss'\n",
    "df_loss.plot(title=Model_Loss_plot_title,figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "\n",
    "Model_AUC_plot_title = 'Model AUC'\n",
    "df_auc.plot(title=Model_AUC_plot_title,grid=True,figsize=(12,8)).set(xlabel='Epoch',ylabel='AUC')\n",
    "\n",
    "# 绘制accuracy, precision, recall\n",
    "df_accuracy = pd.DataFrame(history.history)\n",
    "df_accuracy[['accuracy','val_accuracy']].plot(title='Model Accuracy',grid=True,figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')\n",
    "df_precision = pd.DataFrame(history.history)\n",
    "df_precision[['precision','val_precision']].plot(title='Model Precision',grid=True,figsize=(12,8)).set(xlabel='Epoch',ylabel='Precision')\n",
    "df_recall = pd.DataFrame(history.history)\n",
    "df_recall[['recall','val_recall']].plot(title='Model Recall',grid=True,figsize=(12,8)).set(xlabel='Epoch',ylabel='Recall')\n",
    "\n",
    "\n",
    "eval_result = cnn_model.evaluate(X_test, Y_test)\n",
    "print(f\"test loss: {round(eval_result[0],4)}, test auc: {round(eval_result[1],4)}, test accuracy: {round(eval_result[2],4)}, test precision: {round(eval_result[3],4)}, test recall: {round(eval_result[4],4),}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
